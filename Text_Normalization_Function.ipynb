{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiran597/statistics_analysis/blob/main/Text_Normalization_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTrZr_Zfhh3S"
      },
      "source": [
        "## TEXT NORMALIZATION FUNCTION\n",
        "\n",
        "Below, we'll define a number of functions that perform various text prep-processing jobs, such as tokenization, lemmatization, etc. <br> After each function is defined, you can test that function by running it on a test sentence (object called test_text). <br> At the end of the notebook, we combine all those function into one function called normalize_corpus.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxjYS9_hh3V"
      },
      "source": [
        "We'll use the sentence below for testing the functions. It has punctuations signs, numbers, HTML markups, and other things to take care of:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsp5JsZMhh3W"
      },
      "outputs": [],
      "source": [
        "test_text = \"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0QdSibohh3f"
      },
      "source": [
        "First, let's get Python modules ready and available (remember, you can always look up the modules in Python documentation to learn about what they do in detail):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_851f4ahh3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56663db-952a-4ed8-bbba-8af546183102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting html.parser\n",
            "  Downloading html-parser-0.2.tar.gz (904 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.11/dist-packages (from html.parser) (3.11)\n",
            "Building wheels for collected packages: html.parser\n",
            "  Building wheel for html.parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html.parser: filename=html_parser-0.2-py3-none-any.whl size=1286 sha256=51698b02e8114b3e39c3e36ae065717a1bb0bfc31472ed24e601b61335b40c7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/aa/98/502b17f7b11e03dfd35a42c1fa4cda6220762f34a90af79904\n",
            "Successfully built html.parser\n",
            "Installing collected packages: html.parser\n",
            "Successfully installed html.parser-0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern3\n",
            "  Downloading pattern3-3.0.0.tar.gz (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from pattern3) (4.13.3)\n",
            "Collecting cherrypy (from pattern3)\n",
            "  Downloading CherryPy-18.10.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting docx (from pattern3)\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting feedparser (from pattern3)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pdfminer3k (from pattern3)\n",
            "  Downloading pdfminer3k-1.3.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting simplejson (from pattern3)\n",
            "  Downloading simplejson-3.20.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting pdfminer.six (from pattern3)\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->pattern3) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->pattern3) (4.12.2)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern3)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting portend>=2.1.1 (from cherrypy->pattern3)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from cherrypy->pattern3) (10.6.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern3)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern3)\n",
            "  Downloading jaraco.collections-5.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx->pattern3) (5.3.1)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx->pattern3) (11.1.0)\n",
            "Collecting sgmllib3k (from feedparser->pattern3)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->pattern3) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->pattern3) (43.0.3)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.11/dist-packages (from pdfminer3k->pattern3) (3.11)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern3)\n",
            "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern3) (1.17.1)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern3)\n",
            "  Downloading tempora-5.8.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern3)\n",
            "  Downloading jaraco.text-4.0.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zc.lockfile->cherrypy->pattern3) (75.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern3) (2.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2.8.2)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern3)\n",
            "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern3)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern3)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (1.17.0)\n",
            "Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.8/349.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.20.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading jaraco.collections-5.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Downloading tempora-5.8.0-py3-none-any.whl (14 kB)\n",
            "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
            "Downloading jaraco.text-4.0.0-py3-none-any.whl (11 kB)\n",
            "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
            "Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Building wheels for collected packages: pattern3, docx, sgmllib3k\n",
            "  Building wheel for pattern3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern3: filename=pattern3-3.0.0-py2.py3-none-any.whl size=18554332 sha256=dfb229ed3ec96e2314c344dc415a5571b94a52895076bc6f946045dacda7bc8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/3b/9e/e2c0eb8db016f849b64ef154ef1679a772b20dee2cbb5a55dc\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53892 sha256=d7c623ccaaf4a11a23073e8fe4bbc2ca791f040f96a02ff6700da3b35f7cb015\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=44c7e94d4c33fd0e8eb6407aa1074509bb6d32ef6a5a8233ccc2fbc82f9b0672\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built pattern3 docx sgmllib3k\n",
            "Installing collected packages: sgmllib3k, zc.lockfile, simplejson, pdfminer3k, jaraco.functools, feedparser, docx, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern3\n",
            "Successfully installed autocommand-2.2.2 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 docx-0.2.4 feedparser-6.0.11 jaraco.collections-5.1.0 jaraco.context-6.0.1 jaraco.functools-4.1.0 jaraco.text-4.0.0 pattern3-3.0.0 pdfminer.six-20240706 pdfminer3k-1.3.4 portend-3.2.0 sgmllib3k-1.0.0 simplejson-3.20.1 tempora-5.8.0 zc.lockfile-3.0.post1\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.5)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyLDAvis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.2)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ],
      "source": [
        "#module for supressing warnings about future changes in Python:\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#module sys helps install modules from inside Jupyter:\n",
        "import sys\n",
        "\n",
        "#html parser for digesting text taken from web-pages\n",
        "!{sys.executable} -m pip install html.parser\n",
        "import html.parser\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "#module for handling regular expressions and special characters\n",
        "import re\n",
        "\n",
        "#Natural Language ToolKit\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "#modules for handling unicode characters and strings\n",
        "import unicodedata\n",
        "import string\n",
        "!{sys.executable} -m pip install pattern3\n",
        "import pattern3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import  CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "#pretty printing\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "!{sys.executable} -m pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZD_F1xkhh3g"
      },
      "source": [
        "Define functions for lemmatization and HTML parsing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lu1J3PVhh3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50758ef5-abd0-4f12-cb1d-b5aded29d2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "html_parser = HTMLParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLBkhkR1hh3g"
      },
      "source": [
        "Defining the list of word contractions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONeixIjehh3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d18a7e8-7b2f-4048-ffa0-50c14c0e58f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "contraction_mapping = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UsNGiPshh3h"
      },
      "source": [
        "Select the list of stopwords from NLTK and amend it by adding more stopwords to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnMb7YGmhh3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4cdf595-a09a-4298-e4f0-f3ca17048162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
        "                                 'tell', 'listen', 'one', 'two', 'three',\n",
        "                                 'four', 'five', 'six', 'seven', 'eight',\n",
        "                                 'nine', 'zero', 'join', 'find', 'make',\n",
        "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
        "                                 'also','would']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5GMhNyOhh3i"
      },
      "source": [
        "Split text into word tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgZQd2Mqhh3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a762235-c4e1-481e-82c1-0efee17c1c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", tokenize_text(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLzMCA6Xhh3i"
      },
      "source": [
        "Expand contractions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lySxONKZhh3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66f4dbc-9cd7-4a52-bc62-a4be521a4fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def expand_contractions(text, contraction_mapping):\n",
        "\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())\n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", expand_contractions(test_text,contraction_mapping))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFT5FmSahh3i"
      },
      "source": [
        "Annotate text tokens with Part-Of-Speach tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xunDWRiThh3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3493f2-641b-46a3-ad3c-341006df8565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def pos_tag_text(text_tokens):\n",
        "    def penn_to_wn_tags(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wn.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wn.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wn.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wn.ADV\n",
        "        else:\n",
        "            return None\n",
        "    tagged_text = nltk.pos_tag(text_tokens)\n",
        "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
        "                         for word, pos_tag in\n",
        "                         tagged_text]\n",
        "    return tagged_lower_text\n",
        "\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", pos_tag_text(tokenize_text(test_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUfxX191hh3j"
      },
      "source": [
        "Lemmatize text based on Part-Of-Speach (POS) tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k59Odkq1hh3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b69ca32-1570-4532-b39e-4c994e5adda5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def lemmatize_text(text):\n",
        "    pos_tagged_text = pos_tag_text(text)\n",
        "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
        "                         else word\n",
        "                         for word, pos_tag in pos_tagged_text]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", lemmatize_text(tokenize_text(test_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCPLMObHhh3j"
      },
      "source": [
        "Remove special characters, such as punctuation marks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU8X8vMIhh3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327c385c-6b6a-4270-8c9c-da5b57441722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def remove_special_characters(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
        "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", remove_special_characters(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3an46pCyhh3k"
      },
      "source": [
        "Get rid of stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q547ut1Dhh3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48663b1-13b9-4b13-cdcc-556f77ef74b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def remove_stopwords(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", remove_stopwords(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUHY0uQFhh3k"
      },
      "source": [
        "Remove all non-text characters (numbers, etc.):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzLPzirAhh3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f079c80b-c692-4c18-a6aa-1d47163d182a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def keep_text_characters(text):\n",
        "    filtered_tokens = []\n",
        "    tokens = tokenize_text(text)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", keep_text_characters(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPoEvkoZhh3k"
      },
      "source": [
        "Clean up HTML markups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2-FQRBLhh3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dab2591-39f0-444b-bb3d-892eed3cc4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.reset()\n",
        "        self.fed = []\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "    def get_data(self):\n",
        "        return ' '.join(self.fed)\n",
        "\n",
        "def strip_html(text):\n",
        "    html_stripper = MLStripper()\n",
        "    html_stripper.feed(text)\n",
        "    return html_stripper.get_data()\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", strip_html(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK5YF6byhh3k"
      },
      "source": [
        "Removing accents from characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI18vT5vhh3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e006f9-b4e6-41b6-d7d7-6e73841dab5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def normalize_accented_characters(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
        "    return text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", normalize_accented_characters(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s64ne-UZhh3l"
      },
      "source": [
        "Putting all functions together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIPmLZDghh3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31a0fc6-0856-43ca-d325-e5cca19ecad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def normalize_corpus(corpus, only_text_chars=True):\n",
        "    normalized_corpus = []\n",
        "    for index, text in enumerate(corpus):\n",
        "        text = normalize_accented_characters(text)\n",
        "        text = html.unescape(text)\n",
        "        text = strip_html(text)\n",
        "        text = expand_contractions(text, contraction_mapping)\n",
        "        text = tokenize_text(text)\n",
        "        text = lemmatize_text(text)\n",
        "        text = remove_special_characters(text)\n",
        "        text = remove_stopwords(text)\n",
        "        if only_text_chars:\n",
        "            text = keep_text_characters(text)\n",
        "        #text = tokenize_text(text)\n",
        "        normalized_corpus.append(text)\n",
        "    return normalized_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN_irUsjhh3l"
      },
      "source": [
        "Create a small corpus consisting of 2 test sentences and testing the normalize_corpus function on it (uncomment the lines below first):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfXN9dSYhh3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3128c2-7a8c-4820-f738-653d2595d6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "test_corpus = [test_text]\n",
        "test_corpus.append(test_text)\n",
        "\n",
        "#print(\"Original:  \", test_corpus,\"\\n\")\n",
        "#print(\"Processed: \", normalize_corpus(test_corpus))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}